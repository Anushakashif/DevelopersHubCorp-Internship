# -*- coding: utf-8 -*-
"""Creditriskmodeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QkaAZmzV5mUvBy5RMW2ow-Z6aHorLJre

### Problem statement

### To predict whether a loan applicant is likely to default on a loan
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

pd.set_option('display.max_columns', None)

sns.set_style('whitegrid')

df = pd.read_csv("/content/german_credit_data.csv")
df

df.describe(include="all").T

df.info()

df["Risk"].value_counts()

df["Job"].unique()

df.isnull().sum()

df.duplicated().sum()

df = df.dropna().reset_index(drop=True)
df

df.columns

df.drop(columns=["Unnamed: 0"], inplace=True)

"""## EDA"""

df[["Age", "Credit amount", "Duration"]].hist(bins = 20, edgecolor = "black")
plt.suptitle("Distribution of Numerical Features")
plt.show()

plt.figure(figsize=(10, 5))
for i, col in enumerate (["Age", "Credit amount", "Duration"]):
  plt.subplot(1, 3, i + 1)
  sns.boxplot(y= df[col], color="skyblue")
  plt.title(col)

plt.tight_layout()
plt.show()

df.query("Duration >= 60")

cat_cols = ["Sex", "Job", "Housing", "Saving accounts", "Checking account", "Purpose"]

plt.figure(figsize=(15, 10))
for i, col in enumerate(cat_cols):
  plt.subplot(3, 3, i + 1)
  sns.countplot(data= df, x = col, hue= col, palette = "Set2", order = df[col].value_counts().index)
  plt.title(f"Distribution of {col}")
  plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Most applicants in the dataset are male who already own a house, but they generally have low savings and limited money in their checking accounts.
# loans are mostly for practical needs like buying a car, electronics, or furniture, while other reasons are much less common.

corr = df[["Age", "Job", "Credit amount", "Duration"]].corr()

corr

sns.heatmap(corr, annot= True, cmap="coolwarm", fmt=".2f")
plt.show()

# There is no strong correlation.
# Credit amount and Duration have a moderate correlation (0.61), as the credit amount increases, the loan duration tends to increase as well.

df.groupby("Job")["Credit amount"].mean()

df.groupby("Sex")["Credit amount"].mean()

pd.pivot_table(df, values= "Credit amount", index= "Housing", columns= "Purpose")

sns.scatterplot(data = df, x  = "Age", y="Credit amount", hue= "Sex", size = "Duration", alpha=0.7, palette="Set1")
plt.title("Credit amount vs Age colored by Sex and sized by Duration")
plt.show()

sns.violinplot(data = df, x="Saving accounts", y="Credit amount",hue="Saving accounts", palette="Set2")
plt.title("Credit Amount Distribution by Saving Accounts")
plt.show()

# The plot shows that people with moderate savings take higher and more varied credit amounts, while other groups mostly borrow smaller amounts.
# All saving groups are right-skewed, large loans are rare but present across categories.

df["Risk"].value_counts(normalize = True) * 100

plt.figure(figsize=(15,4))
for i, col in enumerate(["Age", "Credit amount", "Duration"]):
  plt.subplot(1, 3, i+1)
  sns.boxplot(data =df, x="Risk", y=col, hue="Risk", palette="Set2")
  plt.title(f"{col} by Risk")

plt.tight_layout()
plt.show()

df.groupby("Risk")[["Age", "Credit amount", "Duration"]].mean()

# Bad risk customers take larger credit amounts and longer loan durations, which increases default risk.
# Good risk customers borrow smaller amounts for shorter periods, making them safer despite similar ages.

plt.figure(figsize=(15,10))
for i, col in enumerate(cat_cols):
  plt.subplot(3, 3, i + 1)
  sns.countplot(data = df, x = col, hue= "Risk", palette="Set1", order= df[col].value_counts().index)
  plt.title(f"{col} by Risk")
  plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Male are less likely to be default
# people living in rent/free houses are at higher risk
# On savings/checking accounts we can see the risk decreases with getting to the rich side
# Education loans have a higher proportion of bad risk

"""## Feature Engineering"""

df.columns

features = ["Age", "Sex", "Job", "Housing", "Saving accounts", "Checking account", "Credit amount", "Duration"]

target = "Risk"

df_model = df[features + [target]].copy()

from sklearn.preprocessing import LabelEncoder
import joblib

cat_cols = df_model.select_dtypes(include="object").columns.drop("Risk")

le_dict= {}

for col in cat_cols:
  le = LabelEncoder()
  df_model[col] = le.fit_transform(df_model[col])
  le_dict[col] = le_dict
  joblib.dump(le, f"{col}_encoder.pkl")

le_target = LabelEncoder()

target

df_model[target] = le_target.fit_transform(df_model[target])

df_model[target].value_counts()

joblib.dump(le_target, "target_encoder.pkl")

from sklearn.model_selection import train_test_split

X = df_model.drop(target, axis=1)

y = df_model[target]

X

y

X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)

X_train.shape

X_test.shape

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

def train_model(model, param_grid, X_train, X_test, y_train, y_test):
  grid = GridSearchCV(model, param_grid, cv=5, scoring="accuracy", n_jobs= -1)
  grid.fit(X_train, y_train)
  best_model = grid.best_estimator_
  y_pred = best_model.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  return best_model, acc, grid.best_params_

dt = DecisionTreeClassifier(random_state = 1, class_weight = "balanced")
dt_param_grid = {
    "max_depth": [3,5,7,10, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1,2,4]
}

best_dt, acc_dt, params_dt = train_model(dt, dt_param_grid, X_train, X_test, y_train, y_test)

print("Decision Tree Accuracy", acc_dt)

print("Best parameters", params_dt)

rf = RandomForestClassifier(random_state = 1, class_weight = "balanced", n_jobs= -1)
rf_param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [5, 7, 10, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf" : [1, 2, 4]
}

best_rf, acc_rf, params_rf = train_model(rf, rf_param_grid, X_train, X_test, y_train, y_test)

print("Random Forest Accuracy", acc_rf)

print("Best params", params_rf)

et = ExtraTreesClassifier(random_state = 1, class_weight = "balanced", n_jobs= -1)

et_param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [5, 7, 10, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf" : [1, 2, 4]
}

best_et, acc_et, params_et = train_model(et, et_param_grid, X_train, X_test, y_train, y_test)

print("Extra trees accuracy", acc_et)

print("Best params:", params_et)

xgb = XGBClassifier(random_state = 1, scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum(), use_label_encoder = False, eval_metric = "logloss")

xgb_param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5, 7],
    "learning_ratet": [0.01, 0.1, 0.2],
    "subsample" : [0.7, 1],
    "colsample_bytree": [0.7, 1]
}

best_xgb, acc_xgb, params_xgb = train_model(xgb, xgb_param_grid, X_train, X_test, y_train, y_test)

print("XGB accuracy", acc_xgb)

print("Best params", params_xgb)

best_et.predict(X_test)

joblib.dump(best_et, "extra_trees_credit_model.pkl")